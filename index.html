---
layout: default
title: Continuous Delivery on OpenShift
---

name: inverse
layout: true
class: center, middle
---

class: center

.brand-image-top[![HealthPartnersBig](/openshift-commons-dec-2018/images/hp-1v-4c.png)]

# Continuous Delivery on OpenShift

<hr />

## James McShane, Healthpartners

???

Start slow, strong. There's lots here, so you can't garble early.

Energy is key, you love this stuff and you are good at it.

---
layout: true
name: top-bar
.brand-image[![HealthPartners](/openshift-commons-dec-2018/images/hp-1v-4c.png)]
.twitter-handle[
  `@jmcshane`
]
---

# Who is HealthPartners?

_Our mission is to improve health and well-being in partnership with our members, patients and community_

.center[![Info](/openshift-commons-dec-2018/images/healthpartnersinfo.png)]

???

A little data sheet.

Large non-profit, regional health insurance health care provider in Minnesota

Large enterprise - 26k employees!

Started in 1957 as insurer

---

# Who am I?

* Joined HealthPartners in 2016
* Platform and Architecture group
* Our platform runs HealthPartners.com and our health plan administration

???

* Began our OpenShift journey in November 2016

Platform and Architecture group formed out of a team that operated
specifically in the HP.com space, but then became a group meant to
serve a broader area.

The web team has often been the group to push the envelop here at HP.
Expanding this p&a group has allowed the larger plan side to take advantage
of the new ways of operating that we have developed.

---

# A little timeline

.center[![OurTimeline](/openshift-commons-dec-2018/images/timeline.png)]

???

Began evaluation around time I started Novemeber 2016.
Made a purchase decision in Feb, got to implementation in July
Dec 31 production date, beat it by 6 weeks!
Continue to onboard teams onto the platform.

---

# Our Old World

* Long lead time to production
* Deploys generally at night
* Shared environments
* Difficult to validate large changes
  * 10 months for a version upgrade
* Naturally leads us to container orchestration

???

We've operated this way for a long time and lived with the pain.
These shared environments created significant pain.

Teams would bring two feature films to the office at 10PM for deployment nights
once a quarter.

During the last major upgrade for the old platform, the team worked overnight
into the next day. People started showing up for work the next day!

We didn't operate the platform well due
to the challenges of making changes alongside development work.

---

# Philosophies

* Optimize for speed of delivery on the platform
* Secure credential store
* Good default behavior
* Ephemeral build systems
* Source control practices

???

These five philosophies outline how we have made our decisions
going forward to build and design the pipeline. Each of these
decisions has made an impact on things you will see later in this talk.

---

# ToolChain

![JenkinsLogo](/openshift-commons-dec-2018/images/jenkinslogo.png)
![VaultLogo](/openshift-commons-dec-2018/images/vaultlogo.png)
![BitbucketLogo](/openshift-commons-dec-2018/images/bitbucket.png)
![ArtifactoryLogo](/openshift-commons-dec-2018/images/artifactorylogo.png)
![ServiceNowLogo](/openshift-commons-dec-2018/images/service-now-logo.png)
![FortifyLogo](/openshift-commons-dec-2018/images/fortifylogo.png)
![SonarQubeLogo](/openshift-commons-dec-2018/images/sonarqubelogo.png)
![GitlabLogo](/openshift-commons-dec-2018/images/gitlab-logo.png)
![JiraLogo](/openshift-commons-dec-2018/images/jiralogo.png)
![HipChatLogo](/openshift-commons-dec-2018/images/hipchatlogo.png)
![SpringBootLogo](/openshift-commons-dec-2018/images/springbootlogo.png)

???

* Jenkins,  Bitbucket and Gitlab (long story), Artifactory
* Jira, HipChat, Hashicorp Vault
* Fortify, Sonarqube, ServiceNow

Every company has its own toolchain. For companies today,
it doesn't matter what the tools are, as long as they
are operated in a self-service manner and provide
consistent functionality to the teams using them.

Capabilities:
- Build automation
- Source control
- Artifact Management
- Work Tracking
- Chat
- Secret Management
- Code scanning
- Change control

---

# Developer Viewpoint

* Give control of the tools to the developers in self-service workflows
* Single entrypoint to administrative actions
* No longer need to manage hundreds of jobs on a single Jenkins server
  * Jenkins job configuration now comes from source control!

???

Created a single entrypoint configured by a Jenkinsfile.
Developers don't need to learn Groovy, just how to write
a basic map in Groovy

Lower bar to entry, teams don't need to know how to operate These
tools, but they are given power over them through automated
jobs.

---
class: namedcode

# Developer Viewpoint

* Create your git repository with Jenkinsfile and pipeline.yaml at the root
* `oc login https://openshift-bld1.hp.com && oc project MYPROJECT`
* `oc create -f pipeline.yaml`

##### Jenkinsfile
```groovy
hpPipeline([
  appName: "my-service",
  team: "Cloud"
])
```

???

Created templates to generate these Jenkinsfile and pipeline.yaml files.
Mostly boilerplate code that we have reduced as much as possible.
Working to create automated processes that will validate these files
as well as provide suggestions and capabilities that can migrate these files to new versions.

Lower bar to entry in the platform, then democratize as
you gain skill and capability to participate.

The goal here is to make it easy to do the right thing. An application
with these settings can be safely validated and deployed to production as simple
as this looks.

---

# Developer Viewpoint

.center[![ApplicationView](/openshift-commons-dec-2018/images/deploymentExample.png)]

???

This is the view that application developers see when they are executing their pipeline.
We will explore these steps individually and understand how an application
moves from commit to production in our environment.

---

# The Pipeline

1. Prepare
2. Build
3. Test
4. Pre-deploy
5. Deploy
6. Verify
7. Repeat 4-6

???

Our pipeline attempts to abstract each of these six steps to allow
teams to fill in these components as they need. Within a large
organization, it is critical to allow development teams to innovate within
the platform.

---

# Build Stage

* Goal: Get a validated container image to deploy
* Steps:
  * Package artifact
  * Run unit/component tests
  * Run s2i build from base image
  * Publish image to external registry

???

One of the keys of our pipeline here is to provide flexibility in the build
stage. This means we can start with lots of different s2i
base images, provide many different types of base artifacts, and combine
them to make standard sorts of container images, which need little validation
because most of their layers come from our pre-validated s2i image.

Our low bar to entry here is a spring boot application based on a Java
runtime docker image, built using maven. Of course, all of these options can
be configured!

---

# Testing

* Unit & Component tests run before pre-deployment begins
* Generally JUnit tests, building `@SpringBootTest` component tests to load parts of the application
* H2 Database testing

???

Our team goal is to provide lots of capabilities for building a solid
testing pyramid (Fowler), lots of validation early in the build process, and
good, but small end to end tests and smoke testing of applications
as they are deployed.

Need good validation early to avoid breaking integrated environments.
Our web envrionment is integrated across many teams/components, and
we need a way to validate that overall experience as well.

---

# Pre-Deployment

* Environment approval
* HipChat notification
* Check that the target cluster is available
* Other validations as necessary
  * Static code scan approvals
  * Gather Jira information
  * Change management

???

These steps vary by where the artifact is being deployed.

The bar is *set low to deploy* to the dev environments, but as
your code is promoted up the stack, there are more requirements to
pass in both pre-deployment and post deployment verification

---

# Deployment

* Use the OpenShift API to `apply` objects to the environment
* Two strategies
  * Built in "happy path" - use Jenkins to generate objects
  * More flexible templated path - developers provide OpenShift `Template` object
* Integrate with platform components through labels and annotations

???

This is where the Kubernetes/OpenShift API makes our life easy.
We rev our manifest files and then apply them into the environment.
If things go wrong with this there are other API methods available to use.

This again lowers the bar to entry, but as teams gain complexity
they are able to particpate in building more complex images
and manifest objects.

Exploring how to incoporate projects like Helm and Istio.

We do use these standard labels/annotations to bring in features like
Prometheus and our kube-monkey, again good default behaviors!

---

# Verification

* Verify that the deployment completes successfully
* Post deployment separated into "Phases" and run in parallel
* SonarQube scan phase
* Fortify scan phase
  * Separate scanning from result retrieval for better user experience
* Integration tests in source code repository next to application code
* Zephyr tests from JIRA for manual test phases

???

THIS SLIDE IS NOT ABOUT INTEGRATION TEST TYPES.

This is more generic about how verification takes place.

---

# Verification

* Integration tests use the OpenShift proxy API to target new pods
* Record test results for change management
* Three automated test types:
  * API tests
  * Browser tests (Selenium)
  * Shell scripting validation

???

Talk about development team contributing the selenium test strategy
We're always looking for ways to bring people onto the platform
and improve validation strategies.

---

# Change Management

* ServiceNow is our company-wide audit store
  * Test results
  * JIRA ticket number
  * Git commit info
  * Scan results
  * Deployment time
  * Deployment results
  * Approvers

???

Change management really is the game changer for us. ServiceNow
is our company audit store and we uses it for a significant portion of
our auditing capabilities. We want to be able to officially audit
all of these things with every change.

---

# Change Management

* ServiceNow TableAPI
  * Much of the actions in the ServiceNow UI are available via API
* Standard change request process
  * Change request templates that can be instantiated to changes
  * Our pipeline enforces a standard process = Happy Auditors!

???

All features exposed through API

Standard processes work great! Its a single process loop that
starts from

---

# Change Management

* Created a Jenkins plugin to carry out standard actions
* https://github.com/jenkinsci/service-now-plugin
  * Manage tables
  * Attach files
  * Update state
* Provides a simple entrypoint for making the necessary API calls

???

Published open source plugin

Built an easy way to carry out the necessary interations.

---

# Change Management: Results

* Can go from commit to production in 18 minutes
* Have completed over 10 deploys in a single day
* More than 100 audited production actions per week
* Multiple other companies using our plugin
* Validated our change management approach with our audit/compliance groups

???

The outcomes from this work has made our platform an enticing
destination for development teams that are considering the work
it will take to manage their applications.

Last Friday had 10 production deploys. Its becoming more regular
to see multiple changes every single day, rather than just once
in awhile.

---

# Visualizing the Data

* Created a long term view for everything that happens in the pipeline
  * Approvals
  * Jobs
  * Deployments
  * Validations
  * Success/Failure events
  * Test results

???

Transition is important here.

From production auditing to enabling
development teams and platform teams to understand the health
of the system as a whole. We
also wanted to build more capabilities for visualizing what is
happening across our whole build and deploy system.

Every action that takes place runs through our audit store,
just exposed as a REST API that every point in the pipelines
can call.

---

# Visualizing the Data

.center[![Deployments](/openshift-commons-dec-2018/images/deploymentLog.png)]

???

We get great views like this that allow a broader view by team or across a whole
environment.

Invested heavily in Grafana, just like the OpenShift project has.

---

# Visualizing the Data

.center[![DeploymentCount](/openshift-commons-dec-2018/images/deploymentCount.png)]

???

We can get information like this that tracks how well our developers are
doing within our platform. This gives us an early indication of when problems
may arise and we have tracked this since early september.

---

# Multi-Faceted Approach

* This is one of many of these libraries we have written:
  * Liquibase pipelines
  * ConfigMap and Secret management
  * Namespace management
  * Orchestrate Ansible Tower jobs for OpenShift infrastructure
  * Even manage our Weblogic servers through these pipelines(!)

???

Solving for continuous delivery here has allowed us to solve
other orchestration processes. This has included improving the way
that we operate even our old J2EE environment, because we can
do more consistent operational activities controlled by a pipeline.

---

# Building for the future

* Canary deployments and advanced deployment strategy
* More complete testing strategies
* Include more (CNCF!) platform features such as Envoy, Traeger, etc.
* More complete validation deployment success
* Overlay application data with deployment events


???

How do we build moving forward? Sometimes, automation can lock you
in to certain answers. We have found this to be an incredibly flexible
approach that is ready for the future!

What we have found is that we can now iterate on our processes because
everything is driven through a single source control system.

---

# Open Source

* Working to open-source parts of the pipeline
* Open sourced our testing library
* Our team helps maintain two Jenkins plugins (service-now and prometheus)
* Open sourced a library that translates Prometheus alerts to ServiceNow (Snogo)
* Looking for more ways to contribute to the community

???

This is one of the things that I'm really excited about. HP
never had an open source presence before our group formed and began
doing this work, but we've really accelerated our work in the community
in conjunction with the work we have done on OpenShift.

---

# Takeaways

* Jenkins & OpenShift can be a powerful pairing
* Optimizing for delivery has sped adoption
* Supporting testing is critical
* Audit & compliance are our friends
* Open source is great!
* Come work with us: https://healthpartners.com/careers

???

OpenShift team has built many great integrations to Jenkins that
we use across the board.

Testing is key to any pipeline. You cannot do automated work like this
without significant validation steps.

Great stuff! Audit and compliance are great friends!
---

# References

1. https://github.com/jenkinsci/service-now-plugin
2. https://github.com/HealthPartners/jenkins-shared-library-test-helper
3. https://github.com/HealthPartners/snogo
4. https://jenkins.io/doc/book/pipeline/shared-libraries/
5. https://github.com/jmcshane/openshift-commons-dec-2018
6. https://github.com/HealthPartners/rhug-sept-2018

???

The pipeline code can be found in the openshift-commons repository.

There's also tons of code examples in my talks at the Minneapolis RHUG
from September of this year.
