---
layout: default
title: Continuous Delivery on OpenShift
---

name: inverse
layout: true
class: center, middle
---

class: center

.brand-image-top[![HealthPartnersBig](/openshift-commons-dec-2018/images/hp-1v-4c.png)]

# Continuous Delivery on OpenShift

<hr />

## James McShane, Healthpartners

---
layout: true
name: top-bar
.brand-image[![HealthPartners](/openshift-commons-dec-2018/images/hp-1v-4c.png)]
.twitter-handle[
  `@jmcshane`
]
---

# Who is HealthPartners?

_Our mission is to improve health and well-being in partnership with our members, patients and community_

.center[![Info](/openshift-commons-dec-2018/images/healthpartnersinfo.png)]

???

A little data sheet from the internet.

Non-profit, regional health insurance health care provider in Minnesota

Large enterprise - 26k employees!

Started in 1957

---

# Who am I?

* James McShane
* Joined HealthPartners in 2016
* Platform and Architecture group
* Our platform runs HealthPartners.com and our health plan administration

???

* Began our OpenShift journey in November 2016

Platform and Architecture group formed out of a team that operated
specifically in the HP.com space, but then became a group meant to
serve a broader area.

The web team has often been the group to push the envelop here at HP.
Expanding this p&a group has allowed the larger plan side to take advantage
of the new ways of operating that we have developed.

---

# A little timeline

.center[![OurTimeline](/openshift-commons-dec-2018/images/timeline.png)]

---

# Our J2EE World

* Long lead time to production
* Deploys generally at night
* Shared environments
* Difficult to validate large changes
  * 10 months for a version upgrade
* Naturally leads us to container orchestration

???

We've operated this way for a long time and lived with the pain.
These shared environments created significant pain.
We didn't operate the platform well due
to the challenges of making changes alongside development work.

---

# Philosophies

* Optimize for speed of delivery on the platform
* Secure credentials
* Good default behavior
* Ephemeral build systems
* Source control practices - single mainline

???

These five philosophies outline how we have made our decisions
going forward to build and design the pipeline. Each of these
decisions has made an impact on things you will see later in this talk.

---

# ToolChain

![JenkinsLogo](/openshift-commons-dec-2018/images/jenkinslogo.png)
![VaultLogo](/openshift-commons-dec-2018/images/vaultlogo.png)
![BitbucketLogo](/openshift-commons-dec-2018/images/bitbucket.png)
![ArtifactoryLogo](/openshift-commons-dec-2018/images/artifactorylogo.png)
![ServiceNowLogo](/openshift-commons-dec-2018/images/service-now-logo.png)
![FortifyLogo](/openshift-commons-dec-2018/images/fortifylogo.png)
![SonarQubeLogo](/openshift-commons-dec-2018/images/sonarqubelogo.png)
![GitlabLogo](/openshift-commons-dec-2018/images/gitlab-logo.png)
![JiraLogo](/openshift-commons-dec-2018/images/jiralogo.png)
![HipChatLogo](/openshift-commons-dec-2018/images/hipchatlogo.png)
![SpringBootLogo](/openshift-commons-dec-2018/images/springbootlogo.png)

???

* Jenkins,  Bitbucket and Gitlab (long story), Artifactory
* Jira, HipChat, Hashicorp Vault
* Fortify, Sonarqube, ServiceNow

Every company has its own toolchain. For companies today,
it doesn't matter what the tools are, as long as they
are operated in a self-service manner and provide
consistent functionality to the teams using them.

Capabilities:
- Build automation
- Source control
- Artifact Management
- Work Tracking
- Chat
- Secret Management
- Code scanning
- Change control

---

# Developer Viewpoint

* Give control of the tools to the developers in self-service workflows
* Single entrypoint to administrative actions
* No longer need to manage hundreds of jobs on a single Jenkins server
  * Jenkins job configuration now comes from source control!

???

Created a single entrypoint configured by a Jenkinsfile.
Developers don't need to learn Groovy, just how to write
a basic map in Groovy

---
class: namedcode

# Developer Viewpoint

* Create your git repository with Jenkinsfile and pipeline.yaml at the root
* `oc login https://openshift-bld1.hp.com && oc project MYPROJECT`
* `oc create -f pipeline.yaml`

##### Jenkinsfile
```groovy
hpPipeline([
  appName: "my-service",
  team: "Cloud"
])
```

???

Created templates to generate these Jenkinsfile and pipeline.yaml files.
Mostly boilerplate code that we have reduced as much as possible.
Working to create automated processes that will validate these files
as well as provide suggestions and capabilities that can migrate these files to new versions.

The goal here is to make it easy to do the right thing. An application
with these settings can be safely validated and deployed to production as simple
as this looks.

---

# Developer Viewpoint

.center[![ApplicationView](/openshift-commons-dec-2018/images/deploymentExample.png)]

???

This is the view that application developers see when they are executing their pipeline.
---

# The Pipeline

1. Prepare
2. Build
3. Test
4. Pre-deploy
5. Deploy
6. Verify
7. Repeat 4-6

???

Our pipeline attempts to abstract each of these six steps to allow
teams to fill in these components as they need. Within a large
organization, it is critical to allow development teams to innovate within
the platform.

---

# Build Stage

* Goal: Get a validated container image to deploy
* Steps:
  * Package artifact
  * Run unit/component tests
  * Run s2i build from base image
  * Publish image to external registry

???

One of the keys of our pipeline here is to provide flexibility in the build
stage. This means we can start with lots of different s2i
base images, provide many different types of base artifacts, and combine
them to make standard sorts of container images, which need little validation
because most of their layers come from our pre-validated s2i image.

---

# Testing

* Unit & Component tests run before pre-deployment begins
* Generally JUnit tests, building `@SpringBootTest` component tests to load parts of the application
* H2 Database testing

???

Our team goal is to provide lots of capabilities for building a solid
testing pyramid, lots of validation early in the build process, and
good, but small end to end tests and smoke testing of applications
as they are deployed.

---

# Pre-Deployment

* Environment approval
* Check that the target cluster is available
* Other validations as necessary
  * Static code scan approvals
  * Gather Jira information
  * Change management

???

These steps vary by where the artifact is being deployed.
The bar is set low to deploy to the dev environments, but as
your code is promoted up the stack, there are more requirements to
pass in both pre-deployment and post deployment verification
---

# Deployment

* Use the OpenShift API to `apply` objects to the environment
* Two strategies
  * Built in "happy path" - use Jenkins to generate objects
  * More flexible templated path - developers provide OpenShift `Template` object

???

This is where the Kubernetes/OpenShift API makes our life easy.
We rev our manifest files and then apply them into the environment.
If things go wrong with this there are other API methods available to use.

This again lowers the bar to entry, but as teams gain complexity
they are able to particpate in building more complex images
and manifest objects.

Exploring how to incoporate projects like Helm and Istio.

---

# Verification

* Post deployment separated into "Phases" and run in parallel
* SonarQube scan phase
* Fortify scan phase
  * Separate scanning from result retrieval for better user experience
* Integration tests in source code repository next to application code
* Zephyr tests from JIRA for manual test phases

???

THIS SLIDE IS NOT ABOUT INTEGRATION TEST TYPES.

This is more generic about how verification takes place.

---

# Verification

* Integration tests use the OpenShift proxy API to target new pods
* Record test results for change management
* Three automated test types:
  * API tests
  * Browser tests (Selenium)
  * Shell scripting validation

???

Talk about development team contributing the selenium test strategy
We're always looking for ways to bring people onto the platform
and improve validation strategies.

---

# Change Management

* ServiceNow is our company-wide audit store
  * Test results
  * JIRA ticket number
  * Git commit info
  * Scan results
  * Deployment time
  * Deployment results
  * Approvers

???

Change management really is the game changer for us. ServiceNow
is our company audit store and we uses it for a significant portion of
our auditing capabilities. We want to be able to officially audit
all of these things with every change.

---

# Change Management

* ServiceNow TableAPI
  * Much of the actions in the ServiceNow UI are available via API
* Standard change request process
  * Change request templates that can be instantiated to changes
  * Our pipeline enforces a standard process = Happy Auditors!

???

All features exposed through API

Standard processes work great!

---

# Change Management

* Created a Jenkins plugin to carry out standard actions
* https://github.com/jenkinsci/service-now-plugin
  * Manage tables
  * Attach files
  * Update state
* Provides a simple entrypoint for making the necessary API calls

???

Published open source plugin

Built an easy way to carry out the necessary interations.

---

# Change Management: Results

* Can go from commit to production in 18 minutes
* Have completed over 10 deploys in a single day
* More than 100 audited production actions per week
* Multiple other companies using our plugin
* Validated our change management approach with our audit/compliance groups

???

The outcomes from this work has made our platform an enticing
destination for development teams that are considering the work
it will take to manage their applications.

---

# Visualizing the Data

* Created our own audit datastore and send data to Splunk
* Splunk: Short term event view
* Data store: Long term auditing view of things that happen in the pipeline
  * Approvals
  * Jobs
  * Deployments
  * Validations

???

Transition is important here.

Not only did we make it easy to manage how production works, we
also wanted to build more capabilities for visualizing what is
happening across our whole build and deploy system.

Every action that takes place runs through our audit store,
just exposed as a REST API that every point in the pipelines
can call.

---

# Visualizing the Data

.center[![Deployments](/openshift-commons-dec-2018/images/deploymentLog.png)]

???

We get great views like this that allow a broader view by team or across a whole
environment.

Invested heavily in Grafana, just like the OpenShift project has.

---

# Visualizing the Data

.center[![DeploymentCount](/openshift-commons-dec-2018/images/deploymentCount.png)]

???

We can even get information like this that tracks how well our developers are
doing within our platform. This gives us a first indication of when problems
may arise and we have tracked this since early september.

---

# Multi-Faceted Approach

* This is one of many of these libraries we have written:
  * Liquibase pipelines
  * ConfigMap and Secret management
  * Namespace management
  * Orchestrate Ansible Tower jobs for OpenShift infrastructure
  * Even manage our weblogic servers through these pipelines(!)

???

Solving for continuous delivery here has allowed us to solve
other orchestration processes. This has included improving the way
that we operate even our old J2EE environment, because we can
do more consistent operational activities controlled by a pipeline.

---

# Building for the future

* Canary deployments and advanced deployment strategy
* More complete testing strategies
* Include more (CNCF!) platform features such as Istio, Traeger, etc.
* More complete validation deployment success
* Overlay application data with deployment events


???

What we have found is that we can now iterate on our processes because
everything is driven through a single source control system.

---

# Open Source

* Working to open-source parts of the pipeline
* Open sourced our testing library
* Our team manages two Jenkins plugins (service-now and prometheus)
* Open sourced a library that translates Prometheus alerts to ServiceNow (Snogo)
* Looking for more ways to contribute to the community

???

This is one of the things that I'm really excited about. HP
never had an open source presence before our group formed and began
doing this work, but we've really accelerated our work in the community
in conjunction with the work we have done on OpenShift.

---

# Takeaways

* Jenkins & OpenShift can be a powerful pairing
* Optimizing for delivery has sped adoption
* Supporting testing is critical
* Audit & compliance are our friends
* Open source is great!
* Come work with us: https://healthpartners.com/careers

???

Great stuff! Audit and compliance are great friends!
---

# References

1. https://github.com/jenkinsci/service-now-plugin
2. https://github.com/HealthPartners/jenkins-shared-library-test-helper
3. https://github.com/HealthPartners/snogo
4. https://jenkins.io/doc/book/pipeline/shared-libraries/
5. https://github.com/jmcshane/openshift-commons-dec-2018
6. https://github.com/HealthPartners/rhug-sept-2018

???

The pipeline code can be found in the openshift-commons repository.

There's also tons of code examples in my talks at the Minneapolis RHUG
from September of this year.
